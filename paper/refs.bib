
@article{wani_fed-ehr_2025,
	title = {{FED}-{EHR}: {A} {Privacy}-{Preserving} {Federated} {Learning} {Framework} for {Decentralized} {Healthcare} {Analytics}},
	volume = {14},
	issn = {2079-9292},
	shorttitle = {{FED}-{EHR}},
	url = {https://www.mdpi.com/2079-9292/14/16/3261},
	doi = {10.3390/electronics14163261},
	abstract = {The Internet of Medical Things (IoMT) is revolutionizing healthcare by enabling continuous monitoring and real-time data collection through interconnected medical devices such as wearable sensors and smart health monitors. These devices generate sensitive physiological data, including cardiac signals, glucose levels, and vital signs, that are integrated into electronic health records (EHRs). Machine Learning (ML) and Deep Learning (DL) techniques have shown significant potential for predictive diagnostics and decision support based on such data. However, traditional centralized ML approaches raise significant privacy concerns due to the transmission and aggregation of sensitive health information. Additionally, compliance with data protection regulations, such as the Health Insurance Portability and Accountability Act (HIPAA) and General Data Protection Regulation (GDPR), restricts centralized data sharing and analytics. To address these challenges, this study introduces FED-EHR, a privacy-preserving Federated Learning (FL) framework that enables collaborative model training on distributed EHR datasets without transferring raw data from its source. The framework is implemented using Logistic Regression (LR) and Multi-Layer Perceptron (MLP) models and was evaluated using two publicly available clinical datasets: the UCI Breast Cancer Wisconsin (Diagnostic) dataset and the Pima Indians Diabetes dataset. The experimental results demonstrate that FED-EHR achieves a classification performance comparable to centralized learning, with ROC-AUC scores of 0.83 for the Diabetes dataset and 0.98 for the Breast Cancer dataset using MLP while preserving data privacy by ensuring data locality. These findings highlight the practical feasibility and effectiveness of applying the proposed FL approach in real-world IoMT scenarios, offering a secure, scalable, and regulation-compliant solution for intelligent healthcare analytics.},
	language = {en},
	number = {16},
	urldate = {2025-10-15},
	journal = {Electronics},
	author = {Wani, Rızwan Uz Zaman and Can, Ozgu},
	month = aug,
	year = {2025},
	pages = {3261},
}

@inproceedings{abadi_deep_2016,
	title = {Deep {Learning} with {Differential} {Privacy}},
	url = {http://arxiv.org/abs/1607.00133},
	doi = {10.1145/2976749.2978318},
	abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
	urldate = {2025-10-24},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Abadi, Martín and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	month = oct,
	year = {2016},
	note = {arXiv:1607.00133 [stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {308--318},
}

@misc{papernot_semi-supervised_2017,
	title = {Semi-supervised {Knowledge} {Transfer} for {Deep} {Learning} from {Private} {Training} {Data}},
	url = {http://arxiv.org/abs/1610.05755},
	doi = {10.48550/arXiv.1610.05755},
	abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.},
	urldate = {2025-10-24},
	publisher = {arXiv},
	author = {Papernot, Nicolas and Abadi, Martín and Erlingsson, Úlfar and Goodfellow, Ian and Talwar, Kunal},
	month = mar,
	year = {2017},
	note = {arXiv:1610.05755 [stat]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kairouz_advances_2021,
	title = {Advances and {Open} {Problems} in {Federated} {Learning}},
	url = {http://arxiv.org/abs/1912.04977},
	doi = {10.48550/arXiv.1912.04977},
	abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
	urldate = {2025-10-24},
	publisher = {arXiv},
	author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aurélien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D'Oliveira, Rafael G. L. and Eichner, Hubert and Rouayheb, Salim El and Evans, David and Gardner, Josh and Garrett, Zachary and Gascón, Adrià and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konečný, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancrède and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and Özgür, Ayfer and Pagh, Rasmus and Raykova, Mariana and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tramèr, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
	month = mar,
	year = {2021},
	note = {arXiv:1912.04977 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},}

@misc{mohassel_secureml_2017,
	title = {{SecureML}: {A} {System} for {Scalable} {Privacy}-{Preserving} {Machine} {Learning}},
	shorttitle = {{SecureML}},
	url = {https://eprint.iacr.org/2017/396},
	abstract = {Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns.

In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to nonlinear functions such as sigmoid and softmax that are superior to prior work.

We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.},
	urldate = {2025-10-24},
	author = {Mohassel, Payman and Zhang, Yupeng},
	year = {2017},
	note = {Publication info: Published elsewhere. Minor revision. IEEE Symposium on Security and Privacy 2017},
	keywords = {Privacy-preserving machine learning, secure computation.},
}

@misc{shokri_membership_2017,
	title = {Membership {Inference} {Attacks} against {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/1610.05820},
	doi = {10.48550/arXiv.1610.05820},
	abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.},
	urldate = {2025-10-24},
	publisher = {arXiv},
	author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
	month = mar,
	year = {2017},
	note = {arXiv:1610.05820 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bonawitz_practical_2017,
	title = {Practical {Secure} {Aggregation} for {Privacy} {Preserving} {Machine} {Learning}},
	url = {https://eprint.iacr.org/2017/281},
	abstract = {We design a novel, communication-efficient, failure-robust protocol
for secure aggregation of high-dimensional data. Our protocol allows a
server to compute the sum of large, user-held data vectors from mobile
devices in a secure manner (i.e. without learning each
user's individual contribution), and can be used, for example, in a
federated learning setting, to aggregate user-provided model updates
for a deep neural network. We prove the security of our protocol in
the honest-but-curious and malicious settings, and show that security
is maintained even if an arbitrarily chosen subset of users drop out at
any time. We evaluate the efficiency of our protocol and show, by
complexity analysis and a concrete implementation, that its runtime
and communication overhead remain low even on large data sets and
client pools. For 16-bit input values, our protocol offers
\$1.73{\textbackslash}times\$ communication expansion for \$2{\textasciicircum}\{10\}\$ users and
\$2{\textasciicircum}\{20\}\$-dimensional vectors, and \$1.98{\textbackslash}times\$ expansion for \$2{\textasciicircum}\{14\}\$
users and \$2{\textasciicircum}\{24\}\$-dimensional vectors over sending data in the clear.},
	urldate = {2025-10-24},
	author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
	year = {2017},
	note = {Publication info: Preprint. MINOR revision.},
}

@misc{benaissa_tenseal_2021,
	title = {{TenSEAL}: {A} {Library} for {Encrypted} {Tensor} {Operations} {Using} {Homomorphic} {Encryption}},
	shorttitle = {{TenSEAL}},
	url = {http://arxiv.org/abs/2104.03152},
	doi = {10.48550/arXiv.2104.03152},
	abstract = {Machine learning algorithms have achieved remarkable results and are widely applied in a variety of domains. These algorithms often rely on sensitive and private data such as medical and financial records. Therefore, it is vital to draw further attention regarding privacy threats and corresponding defensive techniques applied to machine learning models. In this paper, we present TenSEAL, an open-source library for Privacy-Preserving Machine Learning using Homomorphic Encryption that can be easily integrated within popular machine learning frameworks. We benchmark our implementation using MNIST and show that an encrypted convolutional neural network can be evaluated in less than a second, using less than half a megabyte of communication.},
	urldate = {2025-10-24},
	publisher = {arXiv},
	author = {Benaissa, Ayoub and Retiat, Bilal and Cebere, Bogdan and Belfedhal, Alaa Eddine},
	month = apr,
	year = {2021},
	note = {arXiv:2104.03152 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{beutel_flower_2022,
	title = {Flower: {A} {Friendly} {Federated} {Learning} {Research} {Framework}},
	shorttitle = {Flower},
	url = {http://arxiv.org/abs/2007.14390},
	doi = {10.48550/arXiv.2007.14390},
	abstract = {Federated Learning (FL) has emerged as a promising technique for edge devices to collaboratively learn a shared prediction model, while keeping their training data on the device, thereby decoupling the ability to do machine learning from the need to store the data in the cloud. However, FL is difficult to implement realistically, both in terms of scale and systems heterogeneity. Although there are a number of research frameworks available to simulate FL algorithms, they do not support the study of scalable FL workloads on heterogeneous edge devices. In this paper, we present Flower -- a comprehensive FL framework that distinguishes itself from existing platforms by offering new facilities to execute large-scale FL experiments and consider richly heterogeneous FL device scenarios. Our experiments show Flower can perform FL experiments up to 15M in client size using only a pair of high-end GPUs. Researchers can then seamlessly migrate experiments to real devices to examine other parts of the design space. We believe Flower provides the community with a critical new tool for FL study and development.},
	urldate = {2025-10-24},
	publisher = {arXiv},
	author = {Beutel, Daniel J. and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Fernandez-Marques, Javier and Gao, Yan and Sani, Lorenzo and Li, Kwing Hei and Parcollet, Titouan and Gusmão, Pedro Porto Buarque de and Lane, Nicholas D.},
	month = mar,
	year = {2022},
	note = {arXiv:2007.14390 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{carlini_extracting_2021,
  author = {Carlini, Nicholas and Tram{\`e}r, Florian and Brown, Tom and Song, Dawn and Oprea, Alina},
  title = {Extracting {Training} {Data} from {Large} {Language} {Models}},
  abstract = {It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.},
  booktitle = {Proceedings of the USENIX Security Symposium (or correct venue)},
  year = {2021},
  language = {en},
}

@article{dwork_algorithmic_2013,
	title = {The {Algorithmic} {Foundations} of {Differential} {Privacy}},
	volume = {9},
	issn = {1551-305X, 1551-3068},
	url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-042},
	doi = {10.1561/0400000042},
	language = {en},
	number = {3-4},
	urldate = {2025-10-24},
	journal = {Foundations and Trends® in Theoretical Computer Science},
	author = {Dwork, Cynthia and Roth, Aaron},
	year = {2013},
	pages = {211--407},
}

@inproceedings{fredrikson_model_2015,
	address = {Denver Colorado USA},
	title = {Model {Inversion} {Attacks} that {Exploit} {Confidence} {Information} and {Basic} {Countermeasures}},
	isbn = {978-1-4503-3832-5},
	url = {https://dl.acm.org/doi/10.1145/2810103.2813677},
	doi = {10.1145/2810103.2813677},
	abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classiﬁers in personalized medicine by Fredrikson et al. [13], adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown.},
	language = {en},
	urldate = {2025-10-24},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	month = oct,
	year = {2015},
	pages = {1322--1333},
}

@inproceedings{dowlin_cryptonets_nodate,
	title = {{CryptoNets}: {Applying} {Neural} {Networks} to {Encrypted} {Data} with {High} {Throughput} and {Accuracy}},
	abstract = {Applying machine learning to a problem which involves medical, ﬁnancial, or other types of sensitive data, not only requires accurate predictions but also careful attention to maintaining data privacy and security. Legal and ethical requirements may prevent the use of cloud-based machine learning solutions for such tasks. In this work, we will present a method to convert learned neural networks to CryptoNets, neural networks that can be applied to encrypted data. This allows a data owner to send their data in an encrypted form to a cloud service that hosts the network. The encryption ensures that the data remains conﬁdential since the cloud does not have access to the keys needed to decrypt it. Nevertheless, we will show that the cloud service is capable of applying the neural network to the encrypted data to make encrypted predictions, and also return them in encrypted form. These encrypted predictions can be sent back to the owner of the secret key who can decrypt them. Therefore, the cloud service does not gain any information about the raw data nor about the prediction it made.},
    booktitle = {Proceedings of the 33rd International Conference on Machine Learning (or correct venue)},
    year = {2016},
	language = {en},
	author = {Dowlin, Nathan and Gilad-Bachrach, Ran and Laine, Kim and Lauter, Kristin and Naehrig, Michael and Wernsing, John},
}

@misc{knott_crypten_2022,
	title = {{CrypTen}: {Secure} {Multi}-{Party} {Computation} {Meets} {Machine} {Learning}},
	shorttitle = {{CrypTen}},
	url = {http://arxiv.org/abs/2109.00984},
	doi = {10.48550/arXiv.2109.00984},
	abstract = {Secure multi-party computation (MPC) allows parties to perform computations on data while keeping that data private. This capability has great potential for machine-learning applications: it facilitates training of machine-learning models on private data sets owned by different parties, evaluation of one party's private model using another party's private data, etc. Although a range of studies implement machine-learning models via secure MPC, such implementations are not yet mainstream. Adoption of secure MPC is hampered by the absence of flexible software frameworks that "speak the language" of machine-learning researchers and engineers. To foster adoption of secure MPC in machine learning, we present CrypTen: a software framework that exposes popular secure MPC primitives via abstractions that are common in modern machine-learning frameworks, such as tensor computations, automatic differentiation, and modular neural networks. This paper describes the design of CrypTen and measure its performance on state-of-the-art models for text classification, speech recognition, and image classification. Our benchmarks show that CrypTen's GPU support and high-performance communication between (an arbitrary number of) parties allows it to perform efficient private evaluation of modern machine-learning models under a semi-honest threat model. For example, two parties using CrypTen can securely predict phonemes in speech recordings using Wav2Letter faster than real-time. We hope that CrypTen will spur adoption of secure MPC in the machine-learning community.},
	urldate = {2025-10-24},
	publisher = {arXiv},
	author = {Knott, Brian and Venkataraman, Shobha and Hannun, Awni and Sengupta, Shubho and Ibrahim, Mark and Maaten, Laurens van der},
	month = sep,
	year = {2022},
	note = {arXiv:2109.00984 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@inproceedings{mcmahan_communication-efficient_2017,
	title = {Communication-{Efficient} {Learning} of {Deep} {Networks} from {Decentralized} {Data}},
	url = {https://proceedings.mlr.press/v54/mcmahan17a.html},
	abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
	language = {en},
	urldate = {2025-10-24},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Aguera y},
	month = apr,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1273--1282},
}

@misc{zhang_state---art_2025,
	title = {State-of-the-{Art} {Approaches} to {Enhancing} {Privacy} {Preservation} of {Machine} {Learning} {Datasets}: {A} {Survey}},
	shorttitle = {State-of-the-{Art} {Approaches} to {Enhancing} {Privacy} {Preservation} of {Machine} {Learning} {Datasets}},
	url = {http://arxiv.org/abs/2404.16847},
	doi = {10.48550/arXiv.2404.16847},
	abstract = {This paper examines the evolving landscape of machine learning (ML) and its profound impact across various sectors, with a special focus on the emerging field of Privacy-preserving Machine Learning (PPML). As ML applications become increasingly integral to industries like telecommunications, financial technology, and surveillance, they raise significant privacy concerns, necessitating the development of PPML strategies. The paper highlights the unique challenges in safeguarding privacy within ML frameworks, which stem from the diverse capabilities of potential adversaries, including their ability to infer sensitive information from model outputs or training data. We delve into the spectrum of threat models that characterize adversarial intentions, ranging from membership and attribute inference to data reconstruction. The paper emphasizes the importance of maintaining the confidentiality and integrity of training data, outlining current research efforts that focus on refining training data to minimize privacy-sensitive information and enhancing data processing techniques to uphold privacy. Through a comprehensive analysis of privacy leakage risks and countermeasures in both centralized and collaborative learning settings, this paper aims to provide a thorough understanding of effective strategies for protecting ML training data against privacy intrusions. It explores the balance between data privacy and model utility, shedding light on privacy-preserving techniques that leverage cryptographic methods, Differential Privacy, and Trusted Execution Environments. The discussion extends to the application of these techniques in sensitive domains, underscoring the critical role of PPML in ensuring the privacy and security of ML systems.},
	urldate = {2025-10-25},
	publisher = {arXiv},
	author = {Zhang, Chaoyu and Li, Shaoyu},
	month = jan,
	year = {2025},
	note = {arXiv:2404.16847 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{xu_privacy-preserving_2021,
	title = {Privacy-{Preserving} {Machine} {Learning}: {Methods}, {Challenges} and {Directions}},
	shorttitle = {Privacy-{Preserving} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2108.04417},
	doi = {10.48550/arXiv.2108.04417},
	abstract = {Machine learning (ML) is increasingly being adopted in a wide variety of application domains. Usually, a well-performing ML model relies on a large volume of training data and high-powered computational resources. Such a need for and the use of huge volumes of data raise serious privacy concerns because of the potential risks of leakage of highly privacy-sensitive information; further, the evolving regulatory environments that increasingly restrict access to and use of privacy-sensitive data add significant challenges to fully benefiting from the power of ML for data-driven applications. A trained ML model may also be vulnerable to adversarial attacks such as membership, attribute, or property inference attacks and model inversion attacks. Hence, well-designed privacy-preserving ML (PPML) solutions are critically needed for many emerging applications. Increasingly, significant research efforts from both academia and industry can be seen in PPML areas that aim toward integrating privacy-preserving techniques into ML pipeline or specific algorithms, or designing various PPML architectures. In particular, existing PPML research cross-cut ML, systems and applications design, as well as security and privacy areas; hence, there is a critical need to understand state-of-the-art research, related challenges and a research roadmap for future research in PPML area. In this paper, we systematically review and summarize existing privacy-preserving approaches and propose a Phase, Guarantee, and Utility (PGU) triad based model to understand and guide the evaluation of various PPML solutions by decomposing their privacy-preserving functionalities. We discuss the unique characteristics and challenges of PPML and outline possible research directions that leverage as well as benefit multiple research communities such as ML, distributed systems, security and privacy.},
	urldate = {2025-10-25},
	publisher = {arXiv},
	author = {Xu, Runhua and Baracaldo, Nathalie and Joshi, James},
	month = sep,
	year = {2021},
	note = {arXiv:2108.04417 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{tran_comprehensive_2024,
	title = {A comprehensive survey and taxonomy on privacy-preserving deep learning},
	volume = {576},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231224001164},
	doi = {10.1016/j.neucom.2024.127345},
	abstract = {Deep learning (DL) has been shown to be very eﬀective for many application domains of machine learning (ML), including image classiﬁcation, voice recognition, natural language processing, and bioinformatics. The success of DL techniques is directly related to the availability of large amounts of training data. However, in many cases, the data are sensitive to the users and should be protected to preserve the privacy. Privacypreserving deep learning (PPDL) has thus become a very active research ﬁeld to ensure the training process and use of DL models are productive without exposing or leaking information about the data.},
	language = {en},
	urldate = {2025-10-25},
	journal = {Neurocomputing},
	author = {Tran, Anh-Tu and Luong, The-Dung and Huynh, Van-Nam},
	month = apr,
	year = {2024},
	pages = {127345},
}

@article{pandya_federated_2023,
	title = {Federated learning for smart cities: {A} comprehensive survey},
	volume = {55},
	issn = {2213-1388},
	shorttitle = {Federated learning for smart cities},
	url = {https://www.sciencedirect.com/science/article/pii/S2213138822010359},
	doi = {10.1016/j.seta.2022.102987},
	abstract = {With the advent of new technologies such as the Artificial Intelligence of Things (AIoT), big data, fog computing, and edge computing, smart city applications have suffered from issues, such as leakage of confidential and sensitive information. To envision smart cities, it will be necessary to integrate federated learning (FL) with smart city applications. FL integration with smart city applications can provide privacy preservation and sensitive information protection. In this paper, we present a comprehensive overview of the current and future developments of FL for smart cities. Furthermore, we highlight the societal, industrial, and technological trends driving FL for smart cities. Then, we discuss the concept of FL for smart cities, and numerous FL integrated smart city applications, including smart transportation systems, smart healthcare, smart grid, smart governance, smart disaster management, smart industries, and UAVs for smart city monitoring, as well as alternative solutions and research enhancements for the future. Finally, we outline and analyze various research challenges and prospects for the development of FL for smart cities.},
	urldate = {2025-10-25},
	journal = {Sustainable Energy Technologies and Assessments},
	author = {Pandya, Sharnil and Srivastava, Gautam and Jhaveri, Rutvij and Babu, M. Rajasekhara and Bhattacharya, Sweta and Maddikunta, Praveen Kumar Reddy and Mastorakis, Spyridon and Piran, Md. Jalil and Gadekallu, Thippa Reddy},
	month = feb,
	year = {2023},
	keywords = {Federated learning, Machine learning, Privacy preservation, Smart city},
	pages = {102987},
}

@misc{ryffel_generic_2018,
	title = {A generic framework for privacy preserving deep learning},
	url = {http://arxiv.org/abs/1811.04017},
	doi = {10.48550/arXiv.1811.04017},
	abstract = {We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning.},
	urldate = {2025-10-25},
	publisher = {arXiv},
	author = {Ryffel, Theo and Trask, Andrew and Dahl, Morten and Wagner, Bobby and Mancuso, Jason and Rueckert, Daniel and Passerat-Palmbach, Jonathan},
	month = nov,
	year = {2018},
	note = {arXiv:1811.04017 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{laine_simple_nodate,
	title = {Simple {Encrypted} {Arithmetic} {Library} 2.3.},
    year = {2017},
	language = {en},
	author = {Laine, Kim},
    note = {Microsoft Research / SEAL documentation},
}

@misc{ionita_fate_nodate,
	title = {{FATE} vs. {SecretFlow}: {A} {Practical} {Comparison} for {Privacy}-{Preserving} {Machine} {Learning}},
	language = {en},
    year = {2025},
	author = {Ionita, Vlad and Kromes, Dr Roland},
}

@misc{goldblum_dataset_2021,
	title = {Dataset {Security} for {Machine} {Learning}: {Data} {Poisoning}, {Backdoor} {Attacks}, and {Defenses}},
	shorttitle = {Dataset {Security} for {Machine} {Learning}},
	url = {http://arxiv.org/abs/2012.10544},
	doi = {10.48550/arXiv.2012.10544},
	abstract = {As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space. In addition to describing various poisoning and backdoor threat models and the relationships among them, we develop their unified taxonomy.},
	urldate = {2025-10-29},
	publisher = {arXiv},
	author = {Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and Madry, Aleksander and Li, Bo and Goldstein, Tom},
	month = mar,
	year = {2021},
	note = {arXiv:2012.10544 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@book{european_parliament_directorate_general_for_parliamentary_research_services_impact_2020,
	address = {LU},
	title = {The impact of the general data protection regulation on artificial intelligence.},
	url = {https://data.europa.eu/doi/10.2861/293},
	language = {en},
	urldate = {2025-10-29},
	publisher = {Publications Office},
	author = {{European Parliament. Directorate General for Parliamentary Research Services.}},
	year = {2020},
}

@inproceedings{zhu_deep_2019,
	title = {Deep {Leakage} from {Gradients}},
	volume = {32},
	url = {https://papers.nips.cc/paper_files/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html},
	abstract = {Passing gradient is a widely used scheme in  modern multi-node learning system (e.g, distributed training, collaborative learning). In a long time, people used to believe that gradients are safe to share: i.e, the training set will not be leaked by gradient sharing.  However, in this paper, we show that we can obtain the private training set from the publicly shared gradients.  The leaking only takes few gradient steps to process and can obtain the original training set instead of look-alike alternatives.  We name this leakage as {\textbackslash}textit\{deep leakage from gradient\}  and practically validate the effectiveness of our algorithm on both computer vision and natural language processing tasks. We empirically show that our attack is much stronger than previous approaches and thereby and raise people's awareness to rethink the gradients' safety. We also discuss some possible strategies to defend this deep leakage.},
	urldate = {2025-10-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhu, Ligeng and Liu, Zhijian and Han, Song},
	year = {2019},
}

@misc{noauthor_nvidia_nodate,
	title = {{NVIDIA} {FLARE} — {NVIDIA} {FLARE} 2.7.0 documentation},
	url = {https://nvflare.readthedocs.io/en/main/index.html},
	urldate = {2025-10-29},
}

@techreport{tabassi_artificial_2023,
	address = {Gaithersburg, MD},
	title = {Artificial {Intelligence} {Risk} {Management} {Framework} ({AI} {RMF} 1.0)},
	url = {http://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf},
	abstract = {As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283), the goal of the AI RMF is to offer a resource to the organizations designing, developing, deploying, or using AI systems to help manage the many risks of AI and promote trustworthy and responsible development and use of AI systems. The Framework is intended to be voluntary, rights-preserving, non-sector specific, and use-case agnostic, providing flexibility to organizations of all sizes and in all sectors and throughout society to implement the approaches in the Framework.   The AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies continue to develop, and to be operationalized by organizations in varying degrees and capacities so society can benefit from AI while also being protected from its potential harms.},
	language = {en},
	number = {NIST AI 100-1},
	urldate = {2025-10-29},
	institution = {National Institute of Standards and Technology (U.S.)},
	author = {Tabassi, Elham},
	month = jan,
	year = {2023},
	doi = {10.6028/NIST.AI.100-1},
	keywords = {NIST},
	pages = {NIST AI 100--1},
}

@misc{xu_dp-fedlora_2025,
	title = {{DP}-{FedLoRA}: {Privacy}-{Enhanced} {Federated} {Fine}-{Tuning} for {On}-{Device} {Large} {Language} {Models}},
	shorttitle = {{DP}-{FedLoRA}},
	url = {http://arxiv.org/abs/2509.09097},
	doi = {10.48550/arXiv.2509.09097},
	abstract = {As on-device large language model (LLM) systems become increasingly prevalent, federated fine-tuning enables advanced language understanding and generation directly on edge devices; however, it also involves processing sensitive, user-specific data, raising significant privacy concerns within the federated learning framework. To address these challenges, we propose DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates LoRA-based adaptation with differential privacy in a communication-efficient setting. Each client locally clips and perturbs its LoRA matrices using Gaussian noise to satisfy (\${\textbackslash}epsilon\$, \${\textbackslash}delta\$)-differential privacy. We further provide a theoretical analysis demonstrating the unbiased nature of the updates and deriving bounds on the variance introduced by noise, offering practical guidance for privacy-budget calibration. Experimental results across mainstream benchmarks show that DP-FedLoRA delivers competitive performance while offering strong privacy guarantees, paving the way for scalable and privacy-preserving LLM deployment in on-device environments.},
	urldate = {2025-11-04},
	publisher = {arXiv},
	author = {Xu, Honghui and Shrestha, Shiva and Chen, Wei and Li, Zhiyuan and Cai, Zhipeng},
	month = sep,
	year = {2025},
	note = {arXiv:2509.09097 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{yousefpour_opacus_2022,
	title = {Opacus: {User}-{Friendly} {Differential} {Privacy} {Library} in {PyTorch}},
	shorttitle = {Opacus},
	url = {http://arxiv.org/abs/2109.12298},
	doi = {10.48550/arXiv.2109.12298},
	abstract = {We introduce Opacus, a free, open-source PyTorch library for training deep learning models with differential privacy (hosted at opacus.ai). Opacus is designed for simplicity, flexibility, and speed. It provides a simple and user-friendly API, and enables machine learning practitioners to make a training pipeline private by adding as little as two lines to their code. It supports a wide variety of layers, including multi-head attention, convolution, LSTM, GRU (and generic RNN), and embedding, right out of the box and provides the means for supporting other user-defined layers. Opacus computes batched per-sample gradients, providing higher efficiency compared to the traditional "micro batch" approach. In this paper we present Opacus, detail the principles that drove its implementation and unique features, and benchmark it against other frameworks for training models with differential privacy as well as standard PyTorch.},
	urldate = {2025-11-04},
	publisher = {arXiv},
	author = {Yousefpour, Ashkan and Shilov, Igor and Sablayrolles, Alexandre and Testuggine, Davide and Prasad, Karthik and Malek, Mani and Nguyen, John and Ghosh, Sayan and Bharadwaj, Akash and Zhao, Jessica and Cormode, Graham and Mironov, Ilya},
	month = aug,
	year = {2022},
	note = {arXiv:2109.12298 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}


@misc{njungle_prismo_2025,
	title = {Prismo: {A} {Decision} {Support} {System} for {Privacy}-{Preserving} {ML} {Framework} {Selection}},
	shorttitle = {Prismo},
	url = {http://arxiv.org/abs/2510.09985},
	doi = {10.48550/arXiv.2510.09985},
	abstract = {Machine learning has become a crucial part of our lives, with applications spanning nearly every aspect of our daily activities. However, using personal information in machine learning applications has sparked significant security and privacy concerns about user data. To address these challenges, different privacy-preserving machine learning (PPML) frameworks have been developed to protect sensitive information in machine learning applications. These frameworks generally attempt to balance design trade-offs such as computational efficiency, communication overhead, security guarantees, and scalability. Despite the advancements, selecting the optimal framework and parameters for specific deployment scenarios remains a complex and critical challenge for privacy and security application developers. We present Prismo, an open-source recommendation system designed to aid in selecting optimal parameters and frameworks for different PPML application scenarios. Prismo enables users to explore a comprehensive space of PPML frameworks through various properties based on user-defined objectives. It supports automated filtering of suitable candidate frameworks by considering parameters such as the number of parties in multi-party computation or federated learning and computation cost constraints in homomorphic encryption. Prismo models every use case into a Linear Integer Programming optimization problem, ensuring tailored solutions are recommended for each scenario. We evaluate Prismo's effectiveness through multiple use cases, demonstrating its ability to deliver best-fit solutions in different deployment scenarios.},
	urldate = {2025-10-25},
	publisher = {arXiv},
	author = {Njungle, Nges Brian and Jahns, Eric and Mastromauro, Luigi and Kayang, Edwin P. and Stojkov, Milan and Kinsy, Michel A.},
	month = oct,
	year = {2025},
	note = {arXiv:2510.09985 [cs]},
	keywords = {Computer Science - Cryptography and Security},
}


@article{njungle_guardianml_2025,
	title = {{GuardianML}: {Anatomy} of {Privacy}-{Preserving} {Machine} {Learning} {Techniques} and {Frameworks}},
	volume = {13},
	issn = {2169-3536},
	shorttitle = {{GuardianML}},
	url = {https://ieeexplore.ieee.org/document/10947759},
	doi = {10.1109/ACCESS.2025.3557228},
	abstract = {Machine learning has become integral to our lives, finding applications in nearly every aspect of our daily routines. However, using personal information in machine learning applications has raised concerns about user data privacy and security. As concerns about data privacy grow, algorithms and techniques for achieving robust privacy-preserving machine learning (PPML) have become a pressing technical challenge. Privacy-preserving machine learning PPML aims to safeguard the confidentiality of both data and models and ensure that sensitive information remains protected during training and inference processes. Different techniques, protocols, libraries, and frameworks have been advanced to enable privacy-preserving machine learning, including implementation trade-offs, computational efficiency, communication overhead minimization, security guarantees, and scalability. However, choosing the proper technique, framework, and corresponding algorithmic or system parameters for a specific deployment instance can be difficult. Various techniques, protocols, libraries, and frameworks have been proposed for PPML, but choosing the right combination along with the appropriate algorithmic or system parameters for a specific deployment instance can be very difficult. In this work, we introduce GuardianML, an open-source recommendation system for selecting the correct parameters and suitable framework for specific use cases of privacy-preserving machine learning PPML. GuardianML allows users to search through a wide range of privacy-preserving machine learning PPML frameworks, techniques, protocols, libraries, and more based on a set of objectives. GuardianML filters potential frameworks based on user-defined criteria, such as the number of parties involved in multi-party computation or the need to minimize communication costs in homomorphic encryption scenarios. The system’s recommendations and optimizations are formulated as a maximization problem using linear integer programming to identify the most suitable solution for various use cases. Moreover, this work thoroughly analyzes and presents seventy relevant frameworks in the system’s database. Additionally, we offer an open-source repository containing practical examples and documentation for some of the frameworks.},
	urldate = {2025-10-25},
	journal = {IEEE Access},
	author = {Njungle, Nges Brian and Jahns, Eric and Wu, Zhenqi and Mastromauro, Luigi and Stojkov, Milan and Kinsy, Michel A.},
	year = {2025},
	keywords = {Privacy, Data models, Computational modeling, Differential privacy, federated learning, Federated learning, homomorphic encryption, Libraries, Machine learning, multi-party computation, privacy preserving machine learning, Protocols, Recommender systems, Surveys, Training, trusted execution environment},
	pages = {61483--61510},
}

